[
["main.html", "AGS paper I simulation results Intro Input Cascade 1.0 Analysis Calibrated vs Random (HSA) Calibrated vs Random (Bliss) Correlation (ensemblewise vs modelwise) Fitness Evolution Cascade 2.0 Analysis Fitness vs performance Reproduce simulation results ROC curves Random model results R session info References", " AGS paper I simulation results John Zobolas Last updated: 22 April, 2020 Intro This report has the AGS-Paper I data analysis and resulting figures. Input Loading libraries: library(DT) library(ggpubr) library(RColorBrewer) library(plotly) library(xfun) library(dplyr) library(tibble) library(emba) library(usefun) library(readr) library(latex2exp) Cascade 1.0 Analysis Performance of automatically parameterized models against published data in (Flobak et al. 2015) Calibrated vs Random (HSA) HSA refers to the synergy method used in Drabme to assess the synergies from Gitsbe We test performance using ROC AUC for both the ensemble-wise and model-wise synergies from Drabme Calibrated models: fitted to steady state Random models: produced via abmlog (see here and used in Drabme with synergy_method: hsa Load results: # &#39;ss&#39; =&gt; calibrated models, &#39;random&#39; =&gt; random models ## HSA results ss_hsa_ensemblewise_file = paste0(&quot;results/hsa/cascade_1.0_ss_50sim_fixpoints_ensemblewise_synergies.tab&quot;) ss_hsa_modelwise_file = paste0(&quot;results/hsa/cascade_1.0_ss_50sim_fixpoints_modelwise_synergies.tab&quot;) random_hsa_ensemblewise_file = paste0(&quot;results/hsa/cascade_1.0_random_ensemblewise_synergies.tab&quot;) random_hsa_modelwise_file = paste0(&quot;results/hsa/cascade_1.0_random_modelwise_synergies.tab&quot;) ss_hsa_ensemblewise_synergies = emba::get_synergy_scores(ss_hsa_ensemblewise_file) ss_hsa_modelwise_synergies = emba::get_synergy_scores(ss_hsa_modelwise_file, file_type = &quot;modelwise&quot;) random_hsa_ensemblewise_synergies = emba::get_synergy_scores(random_hsa_ensemblewise_file) random_hsa_modelwise_synergies = emba::get_synergy_scores(random_hsa_modelwise_file, file_type = &quot;modelwise&quot;) # calculate probability of synergy in the modelwise results ss_hsa_modelwise_synergies = ss_hsa_modelwise_synergies %&gt;% mutate(synergy_prob_ss = synergies/(synergies + `non-synergies`)) random_hsa_modelwise_synergies = random_hsa_modelwise_synergies %&gt;% mutate(synergy_prob_random = synergies/(synergies + `non-synergies`)) observed_synergies_file = paste0(&quot;results/observed_synergies_cascade_1.0&quot;) observed_synergies = get_observed_synergies(observed_synergies_file) # 1 (positive/observed synergy) or 0 (negative/not observed) for all tested drug combinations observed = sapply(random_hsa_modelwise_synergies$perturbation %in% observed_synergies, as.integer) ROC curves # &#39;ew&#39; =&gt; ensemble-wise, &#39;mw&#39; =&gt; model-wise pred_ew_hsa = bind_cols(ss_hsa_ensemblewise_synergies %&gt;% rename(ss_score = score), random_hsa_ensemblewise_synergies %&gt;% select(score) %&gt;% rename(random_score = score), as_tibble_col(observed, column_name = &quot;observed&quot;)) pred_mw_hsa = bind_cols( ss_hsa_modelwise_synergies %&gt;% select(perturbation, synergy_prob_ss), random_hsa_modelwise_synergies %&gt;% select(synergy_prob_random), as_tibble_col(observed, column_name = &quot;observed&quot;)) res_ss_ew = get_roc_stats(df = pred_ew_hsa, pred_col = &quot;ss_score&quot;, label_col = &quot;observed&quot;) res_random_ew = get_roc_stats(df = pred_ew_hsa, pred_col = &quot;random_score&quot;, label_col = &quot;observed&quot;) res_ss_mw = get_roc_stats(df = pred_mw_hsa, pred_col = &quot;synergy_prob_ss&quot;, label_col = &quot;observed&quot;, direction = &quot;&gt;&quot;) res_random_mw = get_roc_stats(df = pred_mw_hsa, pred_col = &quot;synergy_prob_random&quot;, label_col = &quot;observed&quot;, direction = &quot;&gt;&quot;) # Plot ROCs my_palette = RColorBrewer::brewer.pal(n = 9, name = &quot;Set1&quot;) plot(x = res_ss_ew$roc_stats$FPR, y = res_ss_ew$roc_stats$TPR, type = &#39;l&#39;, lwd = 2, col = my_palette[1], main = &#39;ROC curve, Ensemble-wise synergies (HSA)&#39;, xlab = &#39;False Positive Rate (FPR)&#39;, ylab = &#39;True Positive Rate (TPR)&#39;) lines(x = res_random_ew$roc_stats$FPR, y = res_random_ew$roc_stats$TPR, lwd = 2, col = my_palette[2]) legend(&#39;bottomright&#39;, title = &#39;AUC&#39;, col = my_palette[1:2], pch = 19, legend = c(paste(round(res_ss_ew$AUC, digits = 3), &quot;Calibrated&quot;), paste(round(res_random_ew$AUC, digits = 3), &quot;Random&quot;))) grid(lwd = 0.5) abline(a = 0, b = 1, col = &#39;#FF726F&#39;, lty = 2) plot(x = res_ss_mw$roc_stats$FPR, y = res_ss_mw$roc_stats$TPR, type = &#39;l&#39;, lwd = 2, col = my_palette[1], main = &#39;ROC curve, Model-wise synergies (HSA)&#39;, xlab = &#39;False Positive Rate (FPR)&#39;, ylab = &#39;True Positive Rate (TPR)&#39;) lines(x = res_random_mw$roc_stats$FPR, y = res_random_mw$roc_stats$TPR, lwd = 2, col = my_palette[2]) legend(&#39;bottomright&#39;, title = &#39;AUC&#39;, col = my_palette[1:2], pch = 19, legend = c(paste(round(res_ss_mw$AUC, digits = 3), &quot;Calibrated&quot;), paste(round(res_random_mw$AUC, digits = 3), &quot;Random&quot;))) grid(lwd = 0.5) abline(a = 0, b = 1, col = &#39;#FF726F&#39;, lty = 2) ROC AUC sensitivity Investigate combining the synergy results of calibrated and random models How information from the ‘random’ models is augmenting calibrated (to steady state) results? Ensemble-wise scenario: \\(score = calibrated + \\beta \\times random\\) \\(\\beta \\rightarrow +\\infty\\): mostly random model predictions \\(\\beta \\rightarrow -\\infty\\): mostly reverse random model predictions Model-wise scenario: \\((1-w) \\times prob_{ss} + w \\times prob_{rand}, w \\in[0,1]\\) \\(w=0\\): only calibrated model predictions \\(w=1\\): only random model predictions # Ensemble-wise betas = seq(from = -20, to = 20, by = 0.1) auc_values_ew = sapply(betas, function(beta) { pred_ew_hsa = pred_ew_hsa %&gt;% mutate(combined_score = ss_score + beta * random_score) res = get_roc_stats(df = pred_ew_hsa, pred_col = &quot;combined_score&quot;, label_col = &quot;observed&quot;) auc_value = res$AUC }) df_ew = as_tibble(cbind(betas, auc_values_ew)) ggline(data = df_ew, x = &quot;betas&quot;, y = &quot;auc_values_ew&quot;, numeric.x.axis = TRUE, plot_type = &quot;l&quot;, xlab = TeX(&quot;$\\\\beta$&quot;), ylab = &quot;AUC (Area Under ROC Curve)&quot;, title = TeX(&quot;AUC sensitivity to $\\\\beta$ parameter: $calibrated + \\\\beta \\\\times random$&quot;), color = my_palette[2]) + geom_vline(xintercept = 0) + grids() # Model-wise weights = seq(from = 0, to = 1, by = 0.05) auc_values_mw = sapply(weights, function(w) { pred_mw_hsa = pred_mw_hsa %&gt;% mutate(weighted_prob = (1 - w) * pred_mw_hsa$synergy_prob_ss + w * pred_mw_hsa$synergy_prob_random) res = get_roc_stats(df = pred_mw_hsa, pred_col = &quot;weighted_prob&quot;, label_col = &quot;observed&quot;, direction = &quot;&gt;&quot;) auc_value = res$AUC }) df_mw = as_tibble(cbind(weights, auc_values_mw)) ggline(data = df_mw, x = &quot;weights&quot;, y = &quot;auc_values_mw&quot;, numeric.x.axis = TRUE, plot_type = &quot;l&quot;, xlab = TeX(&quot;weight $w$&quot;), ylab = &quot;AUC (Area Under ROC Curve)&quot;, title = TeX(&quot;AUC sensitivity to weighted average score: $(1-w) \\\\times prob_{ss} + w \\\\times prob_{rand}$&quot;), color = my_palette[3]) + grids() Symmetricity (Ensemble-wise): \\(AUC_{\\beta \\rightarrow +\\infty} + AUC_{\\beta \\rightarrow -\\infty} \\approx 1\\) Random models perform worse than calibrated ones There is are \\(\\beta\\) values that can boost the predictive performance of the combined synergy classifier but no \\(w\\) weight in the model-wise case Calibrated vs Random (Bliss) Bliss refers to the synergy method used in Drabme to assess the synergies from Gitsbe We test performance using ROC AUC for both the ensemble-wise and model-wise synergies from Drabme Calibrated models: fitted to steady state Random models: produced via abmlog (see here and used in Drabme with synergy_method: bliss Load results: # &#39;ss&#39; =&gt; calibrated models, &#39;random&#39; =&gt; random models ## Bliss results ss_bliss_ensemblewise_file = paste0(&quot;results/bliss/cascade_1.0_ss_50sim_fixpoints_ensemblewise_synergies.tab&quot;) ss_bliss_modelwise_file = paste0(&quot;results/bliss/cascade_1.0_ss_50sim_fixpoints_modelwise_synergies.tab&quot;) random_bliss_ensemblewise_file = paste0(&quot;results/bliss/cascade_1.0_random_bliss_ensemblewise_synergies.tab&quot;) random_bliss_modelwise_file = paste0(&quot;results/bliss/cascade_1.0_random_bliss_modelwise_synergies.tab&quot;) ss_bliss_ensemblewise_synergies = emba::get_synergy_scores(ss_bliss_ensemblewise_file) ss_bliss_modelwise_synergies = emba::get_synergy_scores(ss_bliss_modelwise_file, file_type = &quot;modelwise&quot;) random_bliss_ensemblewise_synergies = emba::get_synergy_scores(random_bliss_ensemblewise_file) random_bliss_modelwise_synergies = emba::get_synergy_scores(random_bliss_modelwise_file, file_type = &quot;modelwise&quot;) # calculate probability of synergy in the modelwise results ss_bliss_modelwise_synergies = ss_bliss_modelwise_synergies %&gt;% mutate(synergy_prob_ss = synergies/(synergies + `non-synergies`)) random_bliss_modelwise_synergies = random_bliss_modelwise_synergies %&gt;% mutate(synergy_prob_random = synergies/(synergies + `non-synergies`)) ROC curves # &#39;ew&#39; =&gt; ensemble-wise, &#39;mw&#39; =&gt; model-wise pred_ew_bliss = bind_cols(ss_bliss_ensemblewise_synergies %&gt;% rename(ss_score = score), random_bliss_ensemblewise_synergies %&gt;% select(score) %&gt;% rename(random_score = score), as_tibble_col(observed, column_name = &quot;observed&quot;)) pred_mw_bliss = bind_cols( ss_bliss_modelwise_synergies %&gt;% select(perturbation, synergy_prob_ss), random_bliss_modelwise_synergies %&gt;% select(synergy_prob_random), as_tibble_col(observed, column_name = &quot;observed&quot;)) res_ss_ew = get_roc_stats(df = pred_ew_bliss, pred_col = &quot;ss_score&quot;, label_col = &quot;observed&quot;) res_random_ew = get_roc_stats(df = pred_ew_bliss, pred_col = &quot;random_score&quot;, label_col = &quot;observed&quot;) res_ss_mw = get_roc_stats(df = pred_mw_bliss, pred_col = &quot;synergy_prob_ss&quot;, label_col = &quot;observed&quot;, direction = &quot;&gt;&quot;) res_random_mw = get_roc_stats(df = pred_mw_bliss, pred_col = &quot;synergy_prob_random&quot;, label_col = &quot;observed&quot;, direction = &quot;&gt;&quot;) # Plot ROCs plot(x = res_ss_ew$roc_stats$FPR, y = res_ss_ew$roc_stats$TPR, type = &#39;l&#39;, lwd = 2, col = my_palette[1], main = &#39;ROC curve, Ensemble-wise synergies (Bliss)&#39;, xlab = &#39;False Positive Rate (FPR)&#39;, ylab = &#39;True Positive Rate (TPR)&#39;) lines(x = res_random_ew$roc_stats$FPR, y = res_random_ew$roc_stats$TPR, lwd = 2, col = my_palette[2]) legend(&#39;bottomright&#39;, title = &#39;AUC&#39;, col = my_palette[1:2], pch = 19, legend = c(paste(round(res_ss_ew$AUC, digits = 3), &quot;Calibrated&quot;), paste(round(res_random_ew$AUC, digits = 3), &quot;Random&quot;))) grid(lwd = 0.5) abline(a = 0, b = 1, col = &#39;#FF726F&#39;, lty = 2) plot(x = res_ss_mw$roc_stats$FPR, y = res_ss_mw$roc_stats$TPR, type = &#39;l&#39;, lwd = 2, col = my_palette[1], main = &#39;ROC curve, Model-wise synergies (Bliss)&#39;, xlab = &#39;False Positive Rate (FPR)&#39;, ylab = &#39;True Positive Rate (TPR)&#39;) lines(x = res_random_mw$roc_stats$FPR, y = res_random_mw$roc_stats$TPR, lwd = 2, col = my_palette[2]) legend(&#39;bottomright&#39;, title = &#39;AUC&#39;, col = my_palette[1:2], pch = 19, legend = c(paste(round(res_ss_mw$AUC, digits = 3), &quot;Calibrated&quot;), paste(round(res_random_mw$AUC, digits = 3), &quot;Random&quot;))) grid(lwd = 0.5) abline(a = 0, b = 1, col = &#39;#FF726F&#39;, lty = 2) ROC AUC sensitivity Investigate same thing as described in here: # Ensemble-wise betas = seq(from = -20, to = 20, by = 0.1) auc_values_ew = sapply(betas, function(beta) { pred_ew_bliss = pred_ew_bliss %&gt;% mutate(combined_score = ss_score + beta * random_score) res = get_roc_stats(df = pred_ew_bliss, pred_col = &quot;combined_score&quot;, label_col = &quot;observed&quot;) auc_value = res$AUC }) df_ew = as_tibble(cbind(betas, auc_values_ew)) ggline(data = df_ew, x = &quot;betas&quot;, y = &quot;auc_values_ew&quot;, numeric.x.axis = TRUE, plot_type = &quot;l&quot;, xlab = TeX(&quot;$\\\\beta$&quot;), ylab = &quot;AUC (Area Under ROC Curve)&quot;, title = TeX(&quot;AUC sensitivity to $\\\\beta$ parameter: $calibrated + \\\\beta \\\\times random$&quot;), color = my_palette[2]) + geom_vline(xintercept = 0) + grids() # Model-wise weights = seq(from = 0, to = 1, by = 0.05) auc_values_mw = sapply(weights, function(w) { pred_mw_bliss = pred_mw_bliss %&gt;% mutate(weighted_prob = (1 - w) * pred_mw_bliss$synergy_prob_ss + w * pred_mw_bliss$synergy_prob_random) res = get_roc_stats(df = pred_mw_bliss, pred_col = &quot;weighted_prob&quot;, label_col = &quot;observed&quot;, direction = &quot;&gt;&quot;) auc_value = res$AUC }) df_mw = as_tibble(cbind(weights, auc_values_mw)) ggline(data = df_mw, x = &quot;weights&quot;, y = &quot;auc_values_mw&quot;, numeric.x.axis = TRUE, plot_type = &quot;l&quot;, xlab = TeX(&quot;weight $w$&quot;), ylab = &quot;AUC (Area Under ROC Curve)&quot;, title = TeX(&quot;AUC sensitivity to weighted average score: $(1-w) \\\\times prob_{ss} + w \\\\times prob_{rand}$&quot;), color = my_palette[3]) + grids() Symmetricity (Ensemble-wise): \\(AUC_{\\beta \\rightarrow +\\infty} + AUC_{\\beta \\rightarrow -\\infty} \\approx 1\\) Random models perform worse than calibrated ones There is are \\(\\beta\\) values that can boost the predictive performance of the combined synergy classifier but no \\(w\\) weight in the model-wise case Correlation (ensemblewise vs modelwise) data_hsa = bind_cols(as_tibble(1 - normalize_to_range(pred_ew_hsa$ss_score)), pred_mw_hsa %&gt;% select(synergy_prob_ss)) colnames(data_hsa) = c(&quot;ensemblewise&quot;, &quot;modelwise&quot;) ggscatter(data = data_hsa, x = &quot;ensemblewise&quot;, y = &quot;modelwise&quot;, add = &quot;reg.line&quot;, add.params = list(color = &quot;blue&quot;, fill = &quot;lightgray&quot;), title = &quot;Correlation (Ensemble-wise vs Model-wise results, HSA)&quot;, conf.int = TRUE, cor.coef = TRUE, cor.coeff.args = list(method = &quot;pearson&quot;)) data_bliss = bind_cols(as_tibble(1 - normalize_to_range(pred_ew_bliss$ss_score)), pred_mw_bliss %&gt;% select(synergy_prob_ss)) colnames(data_bliss) = c(&quot;ensemblewise&quot;, &quot;modelwise&quot;) ggscatter(data = data_bliss, x = &quot;ensemblewise&quot;, y = &quot;modelwise&quot;, add = &quot;reg.line&quot;, add.params = list(color = &quot;blue&quot;, fill = &quot;lightgray&quot;), title = &quot;Correlation (Ensemble-wise vs Model-wise results, Bliss)&quot;, conf.int = TRUE, cor.coef = TRUE, cor.coeff.args = list(method = &quot;pearson&quot;)) No strong correlation Lots of \\(0\\)’s modelwise + small number of tested perturbations affect the correlation Fitness Evolution TODO Cascade 2.0 Analysis Performance of automatically parameterized models against a new dataset (SINTEF, AGS only) TODO Loading results: # original_file = paste0(res_dir, &quot;cascade_2.0_ss_50sim_fixpoints_ensemblewise_synergies.tab&quot;) # random_file = paste0(res_dir, &quot;cascade_2.0_rand_50sim_fixpoints_ensemblewise_synergies.tab&quot;) # observed_synergies_file = paste0(res_dir, &quot;observed_synergies_cascade_2.0&quot;) # # observed_synergies = emba::get_observed_synergies(observed_synergies_file) # orig_res = emba::get_synergy_scores(original_file) # rand_res = emba::get_synergy_scores(random_file) # 1 or 0 for all tested drug combinations: is it observed (TP) or not (TN)? # observed = sapply(orig_res$perturbation %in% observed_synergies, as.integer) Fitness vs performance The idea here is to generate many training data files from the steady state as used in the simulations above for the AGS, where some of the nodes will have their states flipped to the opposite state (\\(0\\) to \\(1\\) and vice versa). That way, we can train models to different steady states, ranging from ones that differ to just a few nodes states up to a steady state that is the complete reversed version of the one used in the simulations above. Using the gen_training_data.R script, we first choose a few number of flips (\\(11\\) flips) ranging from \\(1\\) to \\(24\\) (all nodes) in the steady state. Then, for each such flipping-nodes value, we generated \\(20\\) new steady states with a randomly chosen set of nodes whose value is going to flip. Thus, in total, \\(205\\) training data files were produced (\\(205 = 9 \\times 20 + 24 + 1\\), where from the \\(11\\) number of flips, the one flip happens for every node and flipping all the nodes simultaneously happens once). Running the script run_druglogics_synergy_training.sh from the druglogics-synergy repository root, we get the simulation results for each of these training data files. The only difference in the cascade 2.0 configuration file was the number of simulations (\\(15\\)) for each training data file and the attractor tool used (biolqm_stable_states). We now load the data from these simulations: Reproduce simulation results ROC curves Install druglogics-synergy Run the script run_druglogics_synergy.sh in the above repo using the configuration settings: simulations: 50 attractor_tool: biolqm_stable_states synergy_method: hsa (also rerun the script chaning the synergy method bliss) Thus you will get a directory per simulation and inside will be several result files. To generate the ROC curves, we use the ensemble-wise and model-wise synergies found in each respective simulation. Random model results The CASCADE 1.0 and 2.0 .sif network files can be found at the directories ags_cascade_1.0 and ags_cascade_2.0 on the druglogics-synergy repository. Run the abmlog for the CASCADE 2.0 topology: java -cp target/abmlog-1.5.0-jar-with-dependencies.jar eu.druglogics.abmlog.RandomBooleanModelGenerator --file=test/cascade_2_0.sif --num=3000 Next, prune the resulting models to only the ones that have 1 stable state (\\(1292\\)) using a simple bash script, while renaming the modelnames inside the files so that they have the string _run_ (mimicking thus the files generated by gitsbe - otherwise drabme fails!) - use the script process_models.sh inside the generated models directory from abmlog. cd pathTo/druglogics-synergy/ags_cascade_2.0 Move the models dir inside the ags_cascade_2.0 dir Use attractor_tool: biolqm_stable_states in the config file Use either synergy_method: hsa or synergy_method: bliss Run drabme via druglogics-synergy: java -cp ../target/synergy-1.2.0-jar-with-dependencies.jar eu.druglogics.drabme.Launcher --project=cascade_2.0_random_hsa --modelsDir=models --drugs=drugpanel --perturbations=perturbations --config=config --modeloutputs=modeloutputs java -cp ../target/synergy-1.2.0-jar-with-dependencies.jar eu.druglogics.drabme.Launcher --project=cascade_2.0_random_bliss --modelsDir=models --drugs=drugpanel --perturbations=perturbations --config=config --modeloutputs=modeloutputs The above procedure is the same for CASCADE 1.0. Changes: - Network file is now the cascade_1_0.sif - The process_models.sh needs a small name change (documented inside the script) - The models directory should be put inside the ags_cascade_1.0 of druglogics-synergy - The drabme command should be run with --project=cascade_1.0_random_hsa and --project=cascade_1.0_random_bliss respectively R session info xfun::session_info() R version 3.6.3 (2020-02-29) Platform: x86_64-pc-linux-gnu (64-bit) Running under: Ubuntu 18.04.4 LTS Locale: LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C Package version: askpass_1.1 assertthat_0.2.1 backports_1.1.6 base64enc_0.1.3 BH_1.72.0.3 bibtex_0.4.2.2 bookdown_0.18 callr_3.4.3 Ckmeans.1d.dp_4.3.2 cli_2.0.2 clipr_0.7.0 colorspace_1.4-1 compiler_3.6.3 cowplot_1.0.0 crayon_1.3.4 crosstalk_1.1.0.1 curl_4.3 data.table_1.12.8 desc_1.2.0 digest_0.6.25 dplyr_0.8.5 DT_0.13 ellipsis_0.3.0 emba_0.1.4 evaluate_0.14 fansi_0.4.1 farver_2.0.3 gbRd_0.4-11 ggplot2_3.3.0 ggpubr_0.2.5 ggrepel_0.8.2 ggsci_2.9 ggsignif_0.6.0 glue_1.4.0 graphics_3.6.3 grDevices_3.6.3 grid_3.6.3 gridExtra_2.3 gtable_0.3.0 hexbin_1.28.1 highr_0.8 hms_0.5.3 htmltools_0.4.0 htmlwidgets_1.5.1 httr_1.4.1 igraph_1.2.5 isoband_0.2.1 jsonlite_1.6.1 knitr_1.28 labeling_0.3 later_1.0.0 latex2exp_0.4.0 lattice_0.20.41 lazyeval_0.2.2 lifecycle_0.2.0 magrittr_1.5 markdown_1.1 MASS_7.3.51.5 Matrix_1.2.18 methods_3.6.3 mgcv_1.8.31 mime_0.9 munsell_0.5.0 nlme_3.1.145 openssl_1.4.1 pillar_1.4.3 pkgbuild_1.0.6 pkgconfig_2.0.3 pkgload_1.0.2 plogr_0.2.0 plotly_4.9.2.1 polynom_1.4.0 praise_1.0.0 prettyunits_1.1.1 processx_3.4.2 promises_1.1.0 ps_1.3.2 purrr_0.3.3 R6_2.4.1 RColorBrewer_1.1-2 Rcpp_1.0.4.6 Rdpack_0.11-1 readr_1.3.1 rje_1.10.15 rlang_0.4.5 rmarkdown_2.1 rprojroot_1.3.2 rstudioapi_0.11 scales_1.1.0 splines_3.6.3 stats_3.6.3 stringi_1.4.6 stringr_1.4.0 sys_3.3 testthat_2.3.2 tibble_3.0.0 tidyr_1.0.2 tidyselect_1.0.0 tinytex_0.21 tools_3.6.3 usefun_0.4.5 utf8_1.1.4 utils_3.6.3 vctrs_0.2.4 viridisLite_0.3.0 visNetwork_2.0.9 withr_2.1.2 xfun_0.12 yaml_2.2.1 References Flobak, Åsmund, Anaïs Baudot, Elisabeth Remy, Liv Thommesen, Denis Thieffry, Martin Kuiper, and Astrid Lægreid. 2015. “Discovery of Drug Synergies in Gastric Cancer Cells Predicted by Logical Modeling.” Edited by Ioannis Xenarios. PLOS Computational Biology 11 (8): e1004426. https://doi.org/10.1371/journal.pcbi.1004426. "]
]

---
title: "AGS paper I simulation results"
author: "[John Zobolas](https://github.com/bblodfon)"
date: "Last updated: `r format(Sys.time(), '%d %B, %Y')`"
description: "AGS paper I simulation results"
url: 'https\://username.github.io/reponame/'
github-repo: "username/reponame"
bibliography: references.bib
link-citations: true
site: bookdown::bookdown_site
---

# Intro {-}

This report has the AGS-Paper I data analysis and resulting figures.

# Input {-}

Loading libraries:
```{r Load libraries, message = FALSE}
library(DT)
library(ggpubr)
library(plotly)
library(xfun)
library(dplyr)
library(tibble)
library(emba)
library(usefun)
library(readr)

# ROC packages
library(ROCR)

# results dir
res_dir = paste0(getwd(), "/results/")
```

# Reproduce simulation results {-}

## ROC curves {-}

- Install `druglogics-synergy`
- Use script `.sh`

Thus you will get a directory per simulation and inside will be several result files.
Here we mainly use the ensemble-wise synergies found in each respective simulation.

## Get purely random models {-}

Run the abmlog for CASCADE 2.0 topology:
```
java -cp target/abmlog-1.5.0-jar-with-dependencies.jar eu.druglogics.abmlog.RandomBooleanModelGenerator --file=test/cascade_2_0.sif --num=3000
```

- Prune the resulting models to only the ones that have 1 stable state ($1292$) using simple bash, while renaming the modelnames inside the files so that they have the string `_run_` (mimicking thus the files generated by gitsbe - otherwise drabme fails!) - use the script `process_models.sh`

```
cd pathTo/druglogics-synergy/ags_cascade_2.0
```
- move the `models` dir inside the `ags_cascade_2.0` dir
- Use attractor_tool: `biolqm_stable_states` in the config file
- Run drabme via `druglogics-synergy`:

```
java -cp ../target/synergy-1.2.0-jar-with-dependencies.jar eu.druglogics.drabme.Launcher --project=cascade_2.0_random --modelsDir=models --drugs=drugpanel --perturbations=perturbations --config=config --modeloutputs=modeloutputs
```

# Cascade 1.0 Analysis {-}

:::{.blue-box}
Performance of automatically parameterized models against published data in PLOS comput biol 2015 Flobak paper
:::

## Bnet Reduced {-}

## BioLQM Fixpoints {-}

```{r ROC}
original_file = paste0(res_dir, "cascade_1.0_ss_50sim_fixpoints_ensemblewise_synergies.tab")
random_file = paste0(res_dir, "cascade_1.0_rand_50sim_fixpoints_ensemblewise_synergies.tab")
observed_synergies_file = paste0(res_dir, "observed_synergies_cascade_1.0")

observed_synergies = emba::get_observed_synergies(observed_synergies_file)
orig_res = emba::get_synergy_scores(original_file)
rand_res = emba::get_synergy_scores(random_file)

# 1 or 0 for all tested drug combinations: is it observed (TP) or not (TN)?
observed = sapply(orig_res$perturbation %in% observed_synergies, as.integer)

pred = bind_cols(orig_res, rand_res %>% select(score), as_tibble(observed))
colnames(pred) = c("preturbation", "ss_score", "rand_score", "observed")

pred = pred %>% 
  mutate(ss_score_norm = (ss_score - mean(ss_score)) / sd(ss_score)) %>%
  mutate(rand_score_norm = (rand_score - mean(rand_score)) / sd(rand_score))

pred = pred %>% 
  mutate(combined_score_1 = ss_score - rand_score) %>%
  mutate(combined_score_2 = ss_score_norm - rand_score_norm)

pred %>% filter(combined_score_1 < -0.058)
```


```{r ROC sensitivity cascade 1.0}
# original + beta * random
betas = seq(from = -20, to = 10, by = 0.1)

auc_values = sapply(betas, function(beta) {
  pred = pred %>% mutate(norm_score = ss_score + (beta * rand_score))
  #pred = pred %>% mutate(norm_score = ss_score_norm + (beta * rand_score_norm))
  res = get_roc_stats(df = pred, pred_col = "norm_score", obs_col = "observed")
  auc_value = res$AUC
})

df = as_tibble(cbind(betas, auc_values))

# ggline(data = df, x = "betas", y = "auc_values", numeric.x.axis = TRUE,
#   title = "AUC sensitivity to beta normalization parameter", xlab = "Beta parameter", ylab = "AUC value",
#   color = RColorBrewer::brewer.pal(3, "Set1")[2])

my.title.font = list(family = "Courier New, monospace", size = 18, color = "black")

plot_ly(data = df, x = ~betas, y = ~auc_values, width = 800,
  type = "scatter", mode = "lines+markers", colors = "Set1") %>%
  layout(title = list(text = "AUC sensitivity to beta normalization parameter", font = my.title.font),
    xaxis = list(title = "Beta parameter"),
    yaxis = list(title = "Area Under ROC curve")) %>%
  config(displayModeBar = FALSE)
```

```{r ROC cascade 1}
#best_beta = -0.7
#pred = pred %>% mutate(norm_score = ss_score + (best_beta * rand_score))
res = get_roc_stats(df = pred, pred_col = "combined_score_2", obs_col = "observed")

plot(x = res$roc_stats$FPR, y = res$roc_stats$TPR,
  type = 'l', lwd = 2, col = '#377EB8', main = 'ROC curve',
  xlab = 'False Positive Rate (FPR)', ylab = 'True Positive Rate (TPR)')
legend('bottomright', legend = round(res$AUC, digits = 3),
  title = 'AUC', col = '#377EB8', pch = 19)
grid()
abline(a = 0, b = 1, col = '#FF726F', lty = 2)
```

# Cascade 2.0 Analysis {-}

:::{.blue-box}
Performance of automatically parameterized models against a new dataset (SINTEF, AGS only)
:::

## AUC sensitivity (ensemblewise) {-}

$$score = original + \beta\times random$$

Using **CASCADE 2.0, 50 simulations, BioLQM fixpoints**:

Loading results:
```{r cascade 2.0 results}
original_file = paste0(res_dir, "cascade_2.0_ss_50sim_fixpoints_ensemblewise_synergies.tab")
random_file = paste0(res_dir, "cascade_2.0_rand_50sim_fixpoints_ensemblewise_synergies.tab")
observed_synergies_file = paste0(res_dir, "observed_synergies_cascade_2.0")

observed_synergies = emba::get_observed_synergies(observed_synergies_file)
orig_res = emba::get_synergy_scores(original_file)
rand_res = emba::get_synergy_scores(random_file)

# 1 or 0 for all tested drug combinations: is it observed (TP) or not (TN)?
observed = sapply(orig_res$perturbation %in% observed_synergies, as.integer)
```

Ensemblewise ROC sensitivy analysis:

```{r ensemble-wise ROC sensitivity analysis}
# original + beta * random
betas = seq(from = -5, to = 5, by = 0.05)

auc_values = sapply(betas, function(beta) {
  value = orig_res$score + (beta * rand_res$score)
  pred = prediction(predictions = 1 - normalize_to_range(value), labels = observed)
  auc_res = performance(prediction.obj = pred, measure = "auc")
  auc_value = unlist(auc_res@y.values)
})

df = as_tibble(cbind(betas, auc_values))

# ggline(data = df, x = "betas", y = "auc_values", numeric.x.axis = TRUE,
#   title = "AUC sensitivity to beta normalization parameter", xlab = "Beta parameter", ylab = "AUC value",
#   color = RColorBrewer::brewer.pal(3, "Set1")[2])

my.title.font = list(family = "Courier New, monospace", size = 18, color = "black")

plot_ly(data = df, x = ~betas, y = ~auc_values, width = 800,
  type = "scatter", mode = "lines+markers", colors = "Set1") %>%
  layout(title = list(text = "AUC sensitivity to beta normalization parameter", font = my.title.font),
    xaxis = list(title = "Beta parameter"),
    yaxis = list(title = "Area Under ROC curve")) %>%
  config(displayModeBar = FALSE)
```

:::{.green-box}
- How information from the 'random' models is augmenting calibrated (to steady state) results?
- $\beta \rightarrow +\infty$: only random model predictions
- $\beta \rightarrow -\infty$: only the *reverse* random model predictions
- $AUC_{\beta \rightarrow +\infty} + AUC_{\beta \rightarrow -\infty} = 1$ (complementary)
- Shape of the curve above means:
  - Random models perform statistically the same as calibrated ones
  - But they are not random, they are from the same *proliferative model pool* (all modeloutput nodes are included in the steady state)
:::

## AUC sensitivity (modelwise) {-}

```{r model-wise ROC sensitivity analysis}
modelwise_synergies_file = paste0(res_dir, "cascade_2.0_ss_50sim_fixpoints_modelwise_synergies.tab")
modelwise_synergies_file_rand = paste0(res_dir, "cascade_2.0_rand_50sim_fixpoints_modelwise_synergies.tab")

# original
modelwise_res = get_synergy_scores(file_name = modelwise_synergies_file, file_type = "modelwise")
modelwise_res = modelwise_res %>% 
  mutate(`synergy_prob_ss` = synergies/(synergies + `non-synergies`)) # add synergy probability

# random
modelwise_res_rand = get_synergy_scores(file_name = modelwise_synergies_file_rand, file_type = "modelwise")
modelwise_res_rand = modelwise_res_rand %>% 
  mutate(`synergy_prob_rand` = synergies/(synergies + `non-synergies`)) # add synergy probability
```

We can combine original and random via a weighted probability score:
$$(1-w) \times prob_{ss} + w \times prob_{rand}, w \in[0,1]$$
- More $w$, more random in the game
- Range: **from original only to random only**

```{r model-wise ROC sensitivity analysis, probability-weight based}
# weighted average
weights = seq(from = 0, to = 1, by = 0.05)

pred_modelwise = bind_cols(modelwise_res %>% select(perturbation),
  modelwise_res %>% select(synergy_prob_ss), 
  modelwise_res_rand %>% select(synergy_prob_rand),
  as_tibble_col(observed, column_name = "observed"))

auc_values = sapply(weights, function(w) {
  pred_modelwise = pred_modelwise %>% mutate(weighted_prob = 1 - (
      (1 - w) * pred_modelwise$synergy_prob_ss + w * pred_modelwise$synergy_prob_rand
  ))
  res = get_roc_stats(df = pred_modelwise, pred_col = "weighted_prob", obs_col = "observed")
  auc_value = res$AUC
})

df = as_tibble(cbind(weights, auc_values))

# ggline(data = df, x = "betas", y = "auc_values", numeric.x.axis = TRUE,
#   title = "AUC sensitivity to beta normalization parameter", xlab = "Beta parameter", ylab = "AUC value",
#   color = RColorBrewer::brewer.pal(3, "Set1")[2])

plot_ly(data = df, x = ~weights, y = ~auc_values, width = 800,
  type = "scatter", mode = "lines+markers", colors = "Set1") %>%
  layout(title = list(text = "AUC sensitivity to weighted average", font = my.title.font),
    xaxis = list(title = "Weight"),
    yaxis = list(title = "Area Under ROC curve")) %>%
  config(displayModeBar = FALSE)
```

## Correlation (ensemblewise vs modelwise) {-}

Results correlate between model-wise and ensemble-wise original (steady-state fitting) results:

```{r Correlation between ensemble-wise and model-wise original results, message=FALSE}
data = bind_cols(as_tibble(1 - normalize_to_range(orig_res$score)), modelwise_res %>% select(synergy_prob_ss))
colnames(data) = c("ensemblewise", "modelwise")

# all(modelwise_res$perturbation == predictions_df$perturbation)
ggscatter(data = data, x = "ensemblewise", y = "modelwise", add = "reg.line",
   add.params = list(color = "blue", fill = "lightgray"),
   conf.int = TRUE, cor.coef = TRUE, cor.coeff.args = list(method = "pearson"))
```

## Compare modelwise vs ensemblewise best ROCs {-}

```{r ROCR test usage - ensemble-wise}
# predictions is positive class probability: 1 = synergy, 0 not (so we have to convert accordingly the ensemble-wise data)
pred = prediction(predictions = 1 - normalize_to_range(orig_res$score - 0.2 * rand_res$score), labels = observed)
perf = performance(prediction.obj = pred, measure = "tpr", x.measure = "fpr") # ROC
plot(perf, main = "ROC (ensemble-wise), best beta")
auc_res = performance(prediction.obj = pred, measure = "auc")
auc_value = unlist(auc_res@y.values)
auc_value
```

```{r ROCR test usage - model-wise}
w = 0.2
pred_modelwise = pred_modelwise %>% mutate(weighted_prob = (1 - w) * pred_modelwise$synergy_prob_ss + w * pred_modelwise$synergy_prob_rand)

pred = prediction(predictions = pred_modelwise %>% pull(weighted_prob), labels = observed)
perf = performance(prediction.obj = pred, measure = "tpr", x.measure = "fpr") # ROC
plot(perf, main = "ROC (model-wise), best beta")

auc_res = performance(prediction.obj = pred, measure = "auc")
auc_value = unlist(auc_res@y.values)
auc_value
```

## ROC analytics for purely random models {-}

```{r ROC stats for purely random models}
random_file_new = paste0(res_dir, "cascade_2.0_random_ensemblewise_synergies.tab")
rand_res_new = emba::get_synergy_scores(random_file_new)

# original + beta * random
betas = seq(from = -5, to = 5, by = 0.05)

auc_values = sapply(betas, function(beta) {
  value = orig_res$score + (beta * rand_res_new$score)
  pred = prediction(predictions = 1 - normalize_to_range(value), labels = observed)
  auc_res = performance(prediction.obj = pred, measure = "auc")
  auc_value = unlist(auc_res@y.values)
})

df = as_tibble(cbind(betas, auc_values))

# ggline(data = df, x = "betas", y = "auc_values", numeric.x.axis = TRUE,
#   title = "AUC sensitivity to beta normalization parameter", xlab = "Beta parameter", ylab = "AUC value",
#   color = RColorBrewer::brewer.pal(3, "Set1")[2])

plot_ly(data = df, x = ~betas, y = ~auc_values, width = 800,
  type = "scatter", mode = "lines+markers", colors = "Set1") %>%
  layout(title = list(text = "AUC sensitivity to beta normalization parameter", font = my.title.font),
    xaxis = list(title = "Beta parameter"),
    yaxis = list(title = "Area Under ROC curve")) %>%
  config(displayModeBar = FALSE)
```

## Fitness vs performance {-}

The idea here is to generate many training data files from the steady state as used in the simulations above for the AGS, where some of the nodes will have their states *flipped* to the opposite state ($0$ to $1$ and vice versa).
That way, we can train models to different steady states, ranging from ones that differ to just a few nodes states up to a steady state that is the complete *reversed* version of the one used in the simulations above.

Using the `gen_training_data.R` script, we first choose a few number of flips ($11$ flips) ranging from $1$ to $24$ (all nodes) in the steady state.
Then, for each such *flipping-nodes* value, we generated $20$ new steady states with a randomly chosen set of nodes whose value is going to flip.
Thus, in total, $205$ training data files were produced ($205 = 9 \times 20 + 24 + 1$, where from the $11$ number of flips, the one flip happens for every node and flipping all the nodes simulataneously happens once).

Running the script `run_druglogics_synergy_training.sh` from the `druglogics-synergy` repository root, we get the simulation results for each of these training data files.
The only difference in the cascade 2.0 configuration file was the number of simulations ($15$) for each training data file and the attractor tool used (`biolqm_stable_states`).

We now load the data from these simulations:


# R session info {-}

```{r session info, comment=""}
xfun::session_info()
```

# References {-}
